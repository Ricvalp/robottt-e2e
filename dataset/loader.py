"""
PyTorch dataset utilities for loading QuickDraw K-shot episodes.
"""

from __future__ import annotations

import os
import warnings
from typing import Dict, List, Optional
from pathlib import Path

import faiss
import numpy as np
import torch
from torch.utils.data import Dataset, get_worker_info

from dataset.episode_builder import EpisodeBuilderSimilarMAML
from dataset.storage import DatasetManifest, SketchStorage, StorageConfig

__all__ = ["QuickDrawEpisodesMAML"]


class QuickDrawEpisodesMAML(Dataset):
    """
    PyTorch dataset that lazily assembles K-shot prompt/query episodes.

    Parameters
    ----------
    root : str
        Root directory containing the processed dataset and manifest.
    split : str
        Dataset split (train/val/test).
    K : int
        Number of prompt examples per episode.
        max_seq_len : Optional[int]
            Maximum allowed number of tokens per episode. When provided,
            the loader will resample episodes until the limit is met (with
            a warning) or fail after several attempts.
    backend : str
        Storage backend to use. Should match preprocessing stage.
    coordinate_mode : str
        `"delta"` (default) for motion deltas or `"absolute"` for absolute positions.
    storage_config : Optional[StorageConfig]
        Optional explicit storage configuration. When omitted, a default is
        derived from arguments.
    seed : Optional[int]
        Base seed for deterministic episode sampling across workers.
    """

    def __init__(
        self,
        root: str,
        *,
        split: str = "train",
        K: int = 5,
        max_seq_len: Optional[int] = None,
        max_query_len: Optional[int] = None,
        max_context_len: Optional[int] = None,
        retry_on_overflow: bool = True,
        backend: str = "lmdb",
        storage_config: Optional[StorageConfig] = None,
        seed: int = 0,
        coordinate_mode: str = "delta",
        index_dir: Optional[str] = None,
        ids_dir: Optional[str] = None,
    ) -> None:
        self.root = root
        self.split = split
        self.k_shot = K
        self.max_seq_len = max_seq_len
        self.max_query_len = max_query_len
        self.max_context_len = max_context_len
        self.retry_on_overflow = retry_on_overflow
        self.seed = seed
        self.coordinate_mode = coordinate_mode
        self.index_dir = index_dir
        self.ids_dir = ids_dir

        manifest_path = os.path.join(root, "DatasetManifest.json")
        if not os.path.exists(manifest_path):
            raise FileNotFoundError(
                f"Manifest not found at {manifest_path}. Build the dataset first."
            )
        self.manifest = DatasetManifest.load(manifest_path)

        if storage_config is None:
            storage_config = StorageConfig(root=root, backend=backend)
        self.storage_config = storage_config

        tmp_sketch_storage = SketchStorage(storage_config, mode="r")
        
        
        all_families_path = Path(self.root) / "all_families.txt"
        if all_families_path.exists():
            with open(all_families_path,"r",encoding="utf-8") as f:
                all_families = f.read().splitlines()
        else:
            all_families = tmp_sketch_storage.families()
            with open(all_families_path,"w",encoding="utf-8") as f:
                f.write("\n".join(all_families))
                
                
        self.family_to_samples: Dict[str, List[str]] = {}

        assigned_families = self._resolve_split_families(all_families)
        for family in assigned_families:
            samples = tmp_sketch_storage.samples_for_family(family)
            if len(samples) > 0:
                self.family_to_samples[family] = samples

        tmp_sketch_storage.close()

        self.family_ids = sorted(self.family_to_samples.keys())
        if not self.family_ids:
            raise RuntimeError(f"No sketches found for split '{split}'.")

        self.base_rng = np.random.RandomState(seed)

        self.sketch_storage = None
        self.builder = None
        self._worker_pid = None

        self.retry_on_overflow = retry_on_overflow
        self.max_retry_attempts = 32
        if self.retry_on_overflow:
            warnings.warn(
                f"QuickDrawEpisodes will resample episodes until they fit "
                f"max_seq_len={self.max_seq_len} (up to {self.max_retry_attempts} attempts).",
                RuntimeWarning,
                stacklevel=2,
            )

        self._episode_space = sum(
            max(0, len(samples) - self.k_shot)
            for samples in self.family_to_samples.values()
        )
        if self._episode_space == 0:
            self._episode_space = len(self.family_ids)

        self.faiss_indices, self.ids = self._load_faiss_indices(index_dir, ids_dir)

    def __getstate__(self):
        """Customize pickling to avoid non-fork-safe resources."""
        state = self.__dict__.copy()
        state["sketch_storage"] = None
        state["builder"] = None
        state["_worker_pid"] = None
        return state

    def __len__(self) -> int:
        """Return the nominal number of unique episode samples.

        This enables PyTorch to size epoch loops even though sampling is stochastic.
        """
        return self._episode_space

    def __getitem__(self, index: int) -> Dict[str, object]:
        """Assemble and return a single K-shot episode as PyTorch tensors.

        Deterministic seeds derived from `index` keep behaviour stable per epoch.
        """

        self._ensure_worker_state()  # Initialize SketchStorage and EpisodeBuilderSimilar per worker

        worker = get_worker_info()
        if worker is None:
            base_seed = (self.seed + index) % (2**32)
        else:
            base_seed = (self.seed + worker.id * 10_000 + index) % (2**32)

        attempts = self.max_retry_attempts if self.retry_on_overflow else 1
        episode = None
        for attempt in range(attempts):
            rng_seed = (base_seed + attempt * 9773) % (2**32)
            rng = np.random.RandomState(rng_seed)
            try:
                episode = self.builder.build_episode(rng=rng)
                break
            except ValueError:
                continue

        if episode is None:
            raise RuntimeError(
                f"Unable to sample an episode â‰¤ {self.max_seq_len} tokens after "
                f"{attempts} attempts."
            )
        return {
            "task": episode.task,
            "family_id": episode.family_id,
            "episode_id": episode.episode_id,
        }

    def set_epoch(self, epoch: int) -> None:
        """Update the base random seed for deterministic epoch-level shuffles."""
        self.seed = (self.seed + epoch * 131) % (2**32)

    def _resolve_split_families(self, family_ids: List[str]) -> List[str]:
        """Return the subset of families assigned to the requested split.

        Defaults to all families if no explicit split map is recorded.
        """
        split_map = self.manifest.config.get("family_split_map")
        if split_map:
            return [
                fam for fam in family_ids if split_map.get(fam, "train") == self.split
            ]
        return family_ids

    def close(self) -> None:
        """Release underlying storage resources.

        Explicit closure is useful when loaders are short-lived CLI tools.
        """
        if self.sketch_storage is not None:
            self.sketch_storage.close()

    def _ensure_worker_state(self) -> None:
        """Ensure per-worker state is initialized for multi-process loading.

        Necessary because SketchStorage and EpisodeBuilderSimilar are not fork-safe.
        """
        pid = os.getpid()
        if self.builder is not None and self._worker_pid == pid:
            return
        if self.sketch_storage is not None:
            self.sketch_storage.close()
        self.sketch_storage = SketchStorage(self.storage_config, mode="r")

        self.builder = EpisodeBuilderSimilarMAML(
            fetch_family=self._fetch_family,
            fetch_sketch=self.sketch_storage.get,
            family_ids=self.family_ids,
            k_shot=self.k_shot,
            max_seq_len=self.max_seq_len,
            max_query_len=self.max_query_len,
            max_context_len=self.max_context_len,
            seed=self.seed,
            coordinate_mode=self.coordinate_mode,
            faiss_indices=self.faiss_indices,
            ids=self.ids,
        )

        self._worker_pid = pid

    def _load_faiss_indices(self, index_dir: str, ids_dir: str):

        faiss_indices = {}
        ids_dict = {}
        for family_id in self.family_ids:
            index_path = os.path.join(index_dir, f"family_{family_id}.index")
            ids_path = os.path.join(ids_dir, f"{family_id}.npy")
            if os.path.exists(index_path) and os.path.exists(ids_path):
                index = faiss.read_index(index_path)
                ids = np.load(ids_path)
                faiss_indices[family_id] = index
                ids_dict[family_id] = ids
            else:
                raise FileNotFoundError(
                    f"Faiss index or ids not found for family '{family_id}' "
                    f"in '{index_dir}' or '{ids_dir}'."
                )
        return faiss_indices, ids_dict

    def _fetch_family(self, family_id: str) -> List[str]:
        return self.family_to_samples[family_id]
